---
title: "COVID-19 Symptom Surveys through Facebook"
author: "Alex Reinhart and Ryan Tibshirani"
date: 2020-08-11
tags: ["symptom surveys"]
output:
  html_document:
    code_folding: hide
---

In collaboration with Facebook, along with a consortium of universities and
public health officials, we've been conducting surveys since April 2020 to
monitor the spread and impact of the COVID-19 pandemic in the United States. Our
survey, advertised through Facebook, gets about 74,000 responses every day.
Respondents provide information about COVID-related symptoms, contacts, risk
factors, and demographics, allowing us to examine county-level trends across the
US. We believe such a combination of *detail* and *scale* has never before been
available in a public health emergency. In this post, we'll explore some of our
initial survey findings, show how you can get access to the data, and highlight
some exciting new directions we're now pursuing.

<!--more-->

**Credits.** *Logan Brooks and Katie Mazaitis wrote the initial code for
processing the survey data and producing estimates. Taylor Arnold and Alex
Reinhart provided huge help with refactoring and improving this codebase. Logan
Brooks, Pratik Patil, Rob Tibshirani, and Ryan Tibshirani developed the
statistical methodology behind the estimates computed from the survey data. The
COVIDcast API, which serves these estimates and is updated daily, is a much
larger effort run by the Delphi group; and our entire engineering team is owed a
lot of credit here. Ryan Tibshirani came up with the idea of running the
surveys, and worked with Facebook to make this a reality. On the Facebook side,
Curtiss Cobb and Jonathan McKay played big roles.*

In collaboration with Facebook, along with a consortium of universities and
public health officials, we've been conducting surveys since April 2020 to
monitor the spread and impact of the COVID-19 pandemic in the United States. Our
survey is advertised through Facebook, but it's run on our own Qualtrics
platform (Facebook never sees any of the survey responses). This is an ongoing
operation and our survey gets about 74,000 responses every day. We believe that
such a combination of *detail* and *scale* has never before been available via
surveys in a public health emergency: we collect info on COVID-related symptoms,
contacts, risk factors, and demographics, enabling us to look at county-level
trends across the US. We make aggregated data publicly available daily through
our [COVIDcast
API](https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html), and we
make (fully de-identified) individual survey responses available to researchers
who agree to [data use
terms](https://dataforgood.fb.com/docs/covid-19-symptom-survey-request-for-data-access/).

In this blog post, we'll explore some of our initial findings with two
COVID-like illness indicators that are derived from our surveys. In terms of
what these surveys have to offer, this is really just the tip of the iceberg ...
and we'll highlight some other exciting directions that we're pursuing now.

## Short Background

Back in March 2020, we began discussions with Facebook about the idea of running
a survey, advertised through their site, to collect real-time, county-level
information on people experiencing COVID-like symptoms. The basic premise was
that we could use this information to construct a leading indicator of COVID
activity in the US. We certainly weren't the first group to think of running
COVID-19 symptom surveys: many groups before us had already deployed such
surveys (like [Covid Near You](https://covidnearyou.org/), [COVID Symptom
Study](https://covid.joinzoe.com/us-2), etc.). But we sought to build a survey
that could (a) grow to be truly massive in size and (b) be sustained at this
size, something we felt was only possible with buy-in from a platform like
Facebook. Fortunately for us, they agreed!

We launched our survey on April 6, 2020. Each day since then, Facebook directs a
random sample of its users each day to our survey, hosted on
[Qualtrics](https://www.qualtrics.com/). As part of our agreement with Facebook,
we receive the data directly from Qualtrics, and Facebook never sees any of the
survey responses. As of this writing, our survey receives an average of over
74,000 responses per day; delivers adequate data (enough for us to create
meaningful estimates) for an average of nearly 1000 counties per week; and has
been taken over 9.5 million times so far. An international survey, in over 50
languages, was [launched soon after by the University of
Maryland](https://covidmap.umd.edu/).

We'll go into detail later about on our survey and our survey-based indicators,
but for now, here's a couple quick maps. On the left is a state-level heatmap of
the estimated percentage of people with COVID symptoms, based on our survey,
averaged over June 15, 2020 to July 15, 2020. On the right is a heatmap of the
number of daily new confirmed COVID-19 cases per 100,000 people, based on data
from [JHU CSSE](https://github.com/CSSEGISandData/COVID-19), averaged over the
same period. We can see some clear similarities in the intensity patterns, which
is certainly a good sanity-check.

```{r, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, cache.comments = TRUE)
knitr::opts_chunk$set(fig.width = 10, fig.height = 5)
```

```{r, message = FALSE}
library(covidcast)
library(dplyr)
library(gridExtra)

# Fetch Facebook % CLI signal and JHU confirmed case incidence
start_date = "2020-06-15"
end_date = "2020-07-15"
df_fb = covidcast_signal("fb-survey", "smoothed_cli",
                         start_date, end_date, geo_type = "state")
df_in = covidcast_signal("jhu-csse", "confirmed_7dav_incidence_prop",
                         start_date, end_date, geo_type = "state")

# For each state, average the signals over all available times
df_fb_avg = df_fb %>% group_by(geo_value) %>% summarize(value = mean(value))
df_in_avg = df_in %>% group_by(geo_value) %>% summarize(value = mean(value))

# Set a bunch of fields so that the data frames know how to plot themselves
df_fb_avg$time_value = df_in_avg$time_value = end_date
df_fb_avg$issue = df_in_avg$issue = end_date
df_fb_avg$direction = df_in_avg$direction = 0
attributes(df_fb_avg)$geo_type = attributes(df_in_avg)$geo_type = "state"
class(df_fb_avg) = c("covidcast_signal", class(df_fb_avg))
class(df_in_avg) = c("covidcast_signal", class(df_in_avg))

# Plot choropleth maps, using the covidcast plotting functionality
subtitle = paste("Averaged over", start_date, "to", end_date)
p1 = plot(df_fb_avg,
          title = "% of people with COVID symptoms, based on Facebook surveys",
          range=c(0,1), choro_params = list(subtitle = subtitle))
p2 = plot(df_in_avg,
          title = "Daily new confirmed COVID-19 cases per 100,000 people",
          range=c(0,30), choro_params = list(subtitle = subtitle))
grid.arrange(p1, p2, nrow = 1)
```

(We did this all with our [covidcast R
package](https://cmu-delphi.github.io/covidcast/covidcastR/)---from fetching the
data from our API, to producing the heatmaps---and it requires only about 15
lines of code. You can click the "code" button to reveal it. We'll cover our 
covidcast [R and Python
packages](https://cmu-delphi.github.io/delphi-epidata/api/covidcast_clients.html)
in a future blog post.)

## Why Run These Surveys?

Now let's unpack the main motivation behind our survey a bit: a person typically
experiences COVID-like symptoms before they seek medical care or a COVID-19
test, so data on how many people are self-reporting COVID-like symptoms in a
given county could be used to construct a **leading indicator** of COVID
activity in that county. And to be clear, it's not just the fact that we're
looking at symptoms that's important here, it's the *way* we're measuring them:
since self-reporting symptoms can be done from home, with no special equipment
needed, the data here wouldn't be subject to the same reporting delays as formal
testing metrics like confirmed COVID-19 case counts. 

(Aside from being delayed, confirmed COVID-19 case counts are actually further
confounded by issues like testing policy and capacity; and self-reported symptom
data shouldn't be subject to the same problems. This is a potentially very 
important point, but much more subtle, and we won't delve into it on this post.)

It's also worth being very clear about what we're *not* able to say with these
surveys:

- Symptoms alone are not sufficient to diagnose coronavirus infections; of
course, COVID-like symptoms can be caused by other conditions, and many true
infections are asymptomatic. Therefore, even if we ignore the issues brought on
by self-reporting and survey sampling, we can't expect the estimates we produce
to be reflective of the true rate of COVID-19 (and they're not intended to).

- Our survey responses come from the population of Facebook users in the US,
which may a sizeable fraction of the US population, but certainly not all of it.
We'll also likely see survey bias because some people on Facebook are more
inclined to take surveys than others. We attempt to correct for both of these
biases using a [statistical reweighting
scheme](https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/fb-survey.html#survey-weighting)),
but these corrections aren't perfect.

- Our symptom data is entirely self-reported (as opposed to, for example, being
reported by a medical professional). Some fraction of the responses could be
erroneous, either because a person misunderstood the question or chose to answer
incorrectly.

To summarize, our survey data can't be used to make absolute statements about
the true prevalence of coronavirus disease in the US; in fact, it shouldn't even
be regarded as a foolproof way of deriving unbiased estimates of the number of
people with COVID-19 *symptoms* in the US (mainly due to the self-reporting
aspect). Nevertheless, as we'll see evidence of below, *changes in self-reported 
symptoms over time* can still be a meaningful reflection of the changes in 
coronavirus infections over time. (And in the best case, it'll be a reflection 
of changes to come some number of days into the future.)

## What's on the Survey?

Our survey has 4 sections and is about 35 questions long. The first section is
short and gathers info on a core set of symptoms used to define a condition
called **COVID-like illness** or **CLI**, which we define as *fever of at least
100 °F, along with cough, shortness of breath, or difficulty breathing*. This
mirrors the standard definition of influenza-like illness or ILI (defined as
fever of at least 100 °F, along with sore throat or cough), and is in line with
the working definition of CLI used by the Centers for Disease Control.

Using data from responses to the survey's first section, we can compute
estimates of two key quantities, in a given location, on a given day:

- % CLI: the percentage of people with COVID-like illness; and

- % CLI-in-community: the percentage of people who *know someone in their
local community* with COVID-like illness.

Details on how we compute the % CLI and % CLI-in-community estimates can be
found in our [COVIDcast signals
documentation](https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/fb-survey.html).

The % CLI-in-community indicator has an interesting backstory: it was sort of a
"happy accident" of an experiment we tried with surveys we were running in
partnership with Google. (We'll cover our Google survey in a future blog post.)
The preliminary results from this experiment were exciting enough that we
decided to add a similar question to our Facebook survey about a week after its
initial launch.

The second section of the survey goes into more detail on symptoms, testing, and
medical-seeking behavior. The third section gathers info on contacts and risk
factors, and the fourth section gathers info on demographics. These survey
questions, taken altogether, provide a wealth of information that we haven't
fully tapped into (but plan to soon---see the end of this blog post). To give
credit where it's due: it was Facebook who initially suggested we run a longer
survey with extensive questions on contacts, risk factors, and demographics. At
first, we wanted to run a short survey with only the questions on COVID-like
illness, and Facebook gathered ideas for more questions from several different
research groups in public health. They assured us that lengthening the survey
wouldn't hurt the the completion rate too much, and they were right: of the
people who complete the first section, over 85% go on to finish the full survey!

Our [survey documentation
site](https://cmu-delphi.github.io/delphi-epidata/symptom-survey/) includes more
details about the survey, including the [full text of every survey
version](https://cmu-delphi.github.io/delphi-epidata/symptom-survey/coding.html).
And yes, researchers can [request
access](https://dataforgood.fb.com/docs/covid-19-symptom-survey-request-for-data-access/)
to (fully de-identified) individual survey responses for research purposes.

## Some Interesting Examples

```{r, include = FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.height = 5)
```

```{r, message = FALSE}
library(ggplot2)

# Fetch Facebook % CLI-in-community signal and JHU confirmed case incidence, 
# now at the county level, which is the default in covidcast_signal()
start_date = "2020-06-01"
end_date = "2020-07-15"
df_fb_cty = covidcast_signal("fb-survey", "smoothed_hh_cmnty_cli",
                             start_date, end_date, geo_type = "county")
df_in_cty = covidcast_signal("jhu-csse", "confirmed_7dav_incidence_prop",
                             start_date, end_date, geo_type = "county")

# Filter down to Miami-Dade County
df_fb_md = df_fb_cty %>% filter(geo_value == name_to_fips("Miami-Dade"))
df_in_md = df_in_cty %>% filter(geo_value == name_to_fips("Miami-Dade"))

# Compute ranges of the two signals 
range1 = df_in_md %>% select("value") %>% range
range2 = df_fb_md %>% select("value") %>% range

# Function to transform from one range to another
trans = function(x, from_range, to_range) {
  (x - from_range[1]) / (from_range[2] - from_range[1]) *
    (to_range[2] - to_range[1]) + to_range[1]
}

# Convenience functions for our two signal ranges
trans12 = function(x) trans(x, range1, range2)
trans21 = function(x) trans(x, range2, range1)

# Transform the combined signal to the incidence range, then stack
# these rowwise into one data frame
df = select(rbind(df_fb_md %>% mutate_at("value", trans21),
                  df_in_md), c("time_value", "value"))
df$signal = c(rep("% CLI-in-community", nrow(df_fb_md)),
              rep("New COVID-19 cases", nrow(df_in_md)))   

# Finally, plot both signals
ggplot(df, aes(x = time_value, y = value)) +
  labs(x = "Date") +
  geom_line(aes(color = signal)) +
  scale_y_continuous(
    name = "Daily new confirmed COVID-19 cases",
    sec.axis = sec_axis(trans = trans12, 
                        name = "% of people who know someone with CLI")
  ) + 
  geom_vline(xintercept = as.numeric(as.Date("2020-06-19")), 
             linetype = 2, color = "gray") + 
  geom_vline(xintercept = as.numeric(as.Date("2020-06-25")), 
             linetype = 2, color = "gray") +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.title = ggplot2::element_blank())
```

TODO data examples where the FB signal is a leading indicator. Start with
Miami-Dade? Then give the top 25 sort of thing?

Should we explain that the survey bias doesn't even have to be spatially
constant; for each county, say, the bias only has to be temporally constant or
even slowly drifting?

## Basic Correlation Analysis

TODO Compute correlations over time at multiple lags, plot them. Hopefully
maximized at some nonzero lag (lead).

Maybe do it on the scale of first differences, rather than values themselves?

## What's Next with the Surveys

You might expect that a survey that reaches tens of thousands of respondents
within the United States daily---and has done so for months during a major
pandemic---could have *many* possible uses beyond simply surveying symptoms. You
would be right. With questions on testing, behavior, medical care, mental
health, and related topics, plus the ability to track changes over time and
across the United States, there are numerous possible research questions.

This is why Delphi and the University of Maryland (for the international
version) offer direct access to the survey data to academic and non-profit
researchers. Any researcher need only submit a one-page description of their
project to a [form managed by
Facebook](https://dataforgood.fb.com/docs/covid-19-symptom-survey-request-for-data-access/),
and once approved, Delphi and the University of Maryland grant access. The data
is, of course, protected by use agreements requiring that researchers not
attempt to identify respondents and not release individual survey responses,
only averages and other aggregates.

So far, over a dozen university research teams have access to Delphi's survey
data, and are using it to study various aspects of the pandemic. We are actively
seeking researchers studying important public health problems to [apply for
access](https://dataforgood.fb.com/docs/covid-19-symptom-survey-request-for-data-access/),
and our goal is to build a network of researchers studying the pandemic through
our survey data.

To aid this effort, we're currently releasing a new version of the survey. Based
on feedback from researchers, public health agencies, the University of Maryland
international survey team, and Facebook, we've added items asking about

- COVID testing (in more detail than the current version---including whether the
  respondent tried to get a test but could not)
- mask wearing
- the types of activities the respondent has done with other people
- mental health and social isolation
- employment
- demographics, including race and education

These items will give us an unprecedented view into how people have responded to
the pandemic, how they have been affected by the pandemic and the resulting
economic downturn, and how specific groups are affected. We hope the mask and
behavior items may help researchers studying how best to prevent the spread of
COVID-19 and help inform forecasts, while mental health and isolation items will
help researchers understand how social distancing and recession have affected
mental health. Together with extended demographic data, this may help inform
policies designed to help those most affected by the pandemic.

This new survey is currently being deployed, and data should become available
in the next few weeks. Detailed data will be available to researchers, while new
aggregates---such as of mask-wearing---will be made public through the
[COVIDcast API](https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html)
and the [COVIDcast interactive map](https://covidcast.cmu.edu/).

---

*[Alex Reinhart](https://www.refsmmat.com) manages Delphi's surveys, and is an
Assistant Teaching Professor in the Department of Statistics & Data Science at
CMU.*

*[Ryan Tibshirani](http://stat.cmu.edu/~ryantibs) is a lead researcher in the
Delphi group, and an Associate Professor in the Department of Statistics & Data
Science and the Machine Learning Department at CMU. He is also an Amazon
Scholar.*
